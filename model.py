import os
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()

client = OpenAI(
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
)

MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")


# ----------------------------
# ğŸ”¹ ìŠ¤íŠ¸ë¦¬ë° LLM (generate_stream)
# ----------------------------
def generate_stream(prompt: str):
    stream = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        stream=True,
        max_tokens=256,
        temperature=0.0
    )

    for chunk in stream:
        choice = chunk.choices[0]
        delta = choice.delta

        # contentë§Œ ìŠ¤íŠ¸ë¦¬ë°
        text = getattr(delta, "content", None)
        if text:
            yield text


# ----------------------------
# ğŸ”¹ ì†Œí˜• LLM (run_small_llm)
# ----------------------------
def run_small_llm(prompt: str, max_new_tokens: int = 16) -> str:
    response = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=max_new_tokens,
        temperature=0.0
    )

    return response.choices[0].message.content.strip()

#
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=os.getenv("HUGGINGFACE_TOKEN"))
# model = AutoModelForCausalLM.from_pretrained(
#     MODEL_NAME,
#     torch_dtype=torch.float16,
#     device_map="auto",
#     token=os.getenv("HUGGINGFACE_TOKEN")
# )
#
# print(model.device)
# print(torch.cuda.get_device_name(0))
# print(torch.cuda.memory_summary())
#
# def generate_stream(prompt: str):
#     inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
#
#     streamer = TextIteratorStreamer(tokenizer, skip_prompt=True)
#     thread = threading.Thread(target=lambda: model.generate(
#         **inputs,
#         max_new_tokens=128,
#         do_sample=False,
#         streamer=streamer,
#         # top_p=0.95,  # do_sample=Falseë©´ ì˜í–¥ ê±°ì˜ ì—†ìŒ
#         # temperature=0.2,  # ë§¤ìš° ë³´ìˆ˜ì 
#         repetition_penalty=1.25,
#         eos_token_id=tokenizer.eos_token_id,
#         pad_token_id=tokenizer.eos_token_id
#     ))
#     thread.start()
#
#     # streamerì—ì„œ í† í°ì´ ë‚˜ì˜¤ëŠ” ì¦‰ì‹œ yield
#     for new_text in streamer:
#         yield new_text
#
#
# def run_small_llm(prompt: str, max_new_tokens: int = 16) -> str:
#     # 1) ì…ë ¥ í† í°í™”
#     inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
#
#     # 2) í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì €ì¥
#     prompt_len = inputs.input_ids.shape[1]
#
#     # 3) LLM í˜¸ì¶œ
#     with torch.no_grad():
#         output = model.generate(
#             **inputs,
#             max_new_tokens=max_new_tokens,
#             do_sample=False,
#             # temperature=0.0,
#             top_p=1.0,
#             eos_token_id=tokenizer.eos_token_id
#         )
#
#     # 4) prompt ì´í›„ ìƒì„±ëœ í† í°ë§Œ ë””ì½”ë”©
#     generated_ids = output[0][prompt_len:]
#     result = tokenizer.decode(generated_ids, skip_special_tokens=True)
#
#     return result.strip()
